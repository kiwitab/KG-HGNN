{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 776,
     "status": "ok",
     "timestamp": 1598545060435,
     "user": {
      "displayName": "Mingming Luo",
      "photoUrl": "",
      "userId": "03419135428294618807"
     },
     "user_tz": -60
    },
    "id": "ukOLd5z9o2NQ",
    "outputId": "63270e5f-59fc-40f6-e13c-1a6c14c65aac"
   },
   "outputs": [],
   "source": [
    "  !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"PM-Day-data.txt\",names=[\"area\",\"cycle_id\",\"hour\",\"position_name\",\"station_code\",\"aqi\",\"quality\" ,\"pm2_5\",\"pm2_5_24h\" ,\"pm10\" ,\"pm10_24h\" , \"so2\",\"so2_24h\" , \"no2\",\"no2_24h\", \"co\",\"co_24h\", \"o3\",\"o3_8h\",\"o3_8h_24h\",\"o3_24h\",\"primary_pollutant\",\"drop\"],header = None,parse_dates=True, index_col='cycle_id',encoding='gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chang = data[data['area']=='常州']\n",
    "data_chang = data_chang.drop('drop',axis=1)\n",
    "data_c = data_chang[data_chang['position_name']=='市监测站']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c = data_c.drop(['area','hour','position_name','station_code','aqi','quality','primary_pollutant'],axis=1)\n",
    "data_c.replace('--',np.nan,inplace=True)\n",
    "data_c = data_c.iloc[11:,:]\n",
    "data_c = data_c.fillna(method='ffill')\n",
    "data_c['pm2_5_24h'] = data_c['pm2_5_24h'].astype(float)\n",
    "data_c['pm10_24h'] = data_c['pm10_24h'].astype(float)\n",
    "data_c['so2_24h'] = data_c['so2_24h'].astype(float)\n",
    "data_c['no2_24h'] = data_c['no2_24h'].astype(float)\n",
    "data_c['co_24h'] = data_c['co_24h'].astype(float)\n",
    "data_c['o3_8h'] = data_c['o3_8h'].astype(float)\n",
    "data_c['o3_8h_24h'] = data_c['o3_8h_24h'].astype(float)\n",
    "data_c['o3_24h'] = data_c['o3_24h'].astype(float)\n",
    "print(data_c.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data_multi_cleaned_10_final.csv\",encoding='gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.iloc[:,0:7]\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_origin=dataset.values\n",
    "print(var_origin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8704,0.6881,0.7141,0.7418,0.6655,0.7921"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_weight = [1,0.9640,0.9625,0.9518,0.9240,0.8905,0.8863,0.8730,0.8719,0.8575,0.8527]\n",
    "location_weight = [val for val in location_weight for i in range(7)]\n",
    "domain_weight = [1,0.8702,0.6893,0.7150,0.7416,0.6655,0.7929]\n",
    "domain_weight = domain_weight*11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_weight = [1,0.9640,0.9625,0.9518,0.9240,0.8905,0.8863,0.8730,0.8719,0.8575,0.8527]\n",
    "location_weight = [val for val in location_weight for i in range(7)]\n",
    "domain_weight = [1,0.8316,0.7083,0.7059,0.7500,0.6060,0.7344]\n",
    "domain_weight = domain_weight*11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(location_weight)\n",
    "print(domain_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [a*b for a,b in zip(data_weight,domain_weight)]\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_d = pd.read_csv('weight.csv', encoding='gb18030')\n",
    "data_weight = np.array(weight_d['weight'])\n",
    "location = np.array(weight_d['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wei = data_weight[1:7]\n",
    "print(data_wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sery = pd.DataFrame(np.ones(3250))\n",
    "del_list = list()\n",
    "weight_list = list()\n",
    "data_w = list()\n",
    "domain = list()\n",
    "loc = list()\n",
    "for i in range(77):\n",
    "    if weight[i] < 0.6:\n",
    "        del_list.append(i)\n",
    "    else:\n",
    "        weight_list.append(weight[i])\n",
    "        data_w.append(data_weight[i])\n",
    "        domain.append(domain_weight[i])\n",
    "        loc.append(location[i])\n",
    "origin = np.delete(var_origin,del_list,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(weight_list))\n",
    "print(len(data_w))\n",
    "print(len(domain))\n",
    "print(origin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loc)\n",
    "print(weight_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hw0hFQeWup8R"
   },
   "source": [
    "Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = var_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scale = scaler.fit_transform(var_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T9bUOj3purhd"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scale = scaler.fit_transform(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_data = pd.DataFrame(scaled)\n",
    "# scaled_data.plot(subplots=True, figsize=(20,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    scale[:,i] = scale[:,i]*weight_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(77):\n",
    "    scale[:,i] = scale[:,i]*data_w[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(77):\n",
    "    scale[:,i] = scale[:,i]*domain[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(24):\n",
    "    scaled[:,i] = scale[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5b-DE_7huwo4"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "var= torch.FloatTensor(scale).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vAm8BF6Nu3eI"
   },
   "source": [
    "Split training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5Z9FaVUu7ji"
   },
   "outputs": [],
   "source": [
    "def splitData(var,per_val,per_test):\n",
    "    num_val=int(len(var)*per_val)\n",
    "    num_test=int(len(var)*per_test)\n",
    "    train_size=int(len(var)-num_val-num_test)\n",
    "    train_data=var[0:train_size]\n",
    "    val_data=var[train_size:train_size+num_val]\n",
    "    test_data=var[train_size+num_val:train_size+num_val+num_test]\n",
    "    return train_data,val_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "12s54oyKu_qb"
   },
   "outputs": [],
   "source": [
    "train_data,val_data,test_data=splitData(var,0.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_data.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16898,
     "status": "ok",
     "timestamp": 1598545076624,
     "user": {
      "displayName": "Mingming Luo",
      "photoUrl": "",
      "userId": "03419135428294618807"
     },
     "user_tz": -60
    },
    "id": "3CL_EC11vDhf",
    "outputId": "9792c70f-ab2d-4c5c-b748-063ba9ee6bd6"
   },
   "outputs": [],
   "source": [
    "print('The length of train data, validation data and test data are:',len(train_data),',',len(val_data),',',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Neov5unqwMkx"
   },
   "outputs": [],
   "source": [
    "train_window = 8\n",
    "def create_train_sequence(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        train_label = input_data[i+tw:i+tw+1,0]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return inout_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16889,
     "status": "ok",
     "timestamp": 1598545076625,
     "user": {
      "displayName": "Mingming Luo",
      "photoUrl": "",
      "userId": "03419135428294618807"
     },
     "user_tz": -60
    },
    "id": "Nq2VNCQZwQNb",
    "outputId": "933c214b-7b3f-41f6-8fb6-eeb122f1fb49"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "train_inout_seq = create_train_sequence(train_data, train_window)\n",
    "print('The total number of train windows is',len(train_inout_seq))\n",
    "train_inout_seq = MyDataset(train_inout_seq)\n",
    "train_inout_seq = DataLoader(dataset=train_inout_seq, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_inout_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (seq,label) in train_inout_seq:\n",
    "    print(seq.shape)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "loss_train_dic={\n",
    "    'BP':[],\n",
    "    'RNN':[],\n",
    "    'LSTM':[],\n",
    "    'CNN':[],\n",
    "    'BiLSTM':[],\n",
    "    'CNNLSTM':[],\n",
    "    'BiLSTMwithAttention':[],\n",
    "    'CNNLSTMwithAttention':[],\n",
    "    'LSTNet':[],\n",
    "}\n",
    "loss_val_dic=loss_train_dic\n",
    "value_train_dic=loss_train_dic\n",
    "value_val_dic=loss_train_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(attr,model):\n",
    "    model.train()\n",
    "    best_model = model\n",
    "    flag = 1\n",
    "    path = './'+attr+'_aqi.pt'\n",
    "    print('train',attr,'model')\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        #train\n",
    "        add=0\n",
    "        count=0\n",
    "        for (seq,label) in train_inout_seq: \n",
    "            count+=1\n",
    "            optimizer_aqi.zero_grad()\n",
    "            label=label.to(device)\n",
    "#             model.hidden_cell = (torch.zeros(2, 1, model.hidden_layer_size).to(device),torch.zeros(2, 1, model.hidden_layer_size).to(device))\n",
    "            y_pred = model(seq)\n",
    "\n",
    "            if(i==epochs-1):  \n",
    "                value_train_dic[attr].append(y_pred)\n",
    "            \n",
    "            single_loss = loss_function(y_pred, label)   \n",
    "            \n",
    "            add+=single_loss\n",
    "            single_loss.backward()\n",
    "            optimizer_aqi.step()\n",
    "        print(\"the learning rate of %d epoch：%f\" % (i, optimizer_aqi.param_groups[0]['lr']))\n",
    "        scheduler.step()\n",
    "        loss_train=add/count\n",
    "        loss_train_dic[attr].append(loss_train)\n",
    "        \n",
    "        #val\n",
    "        add1=0 \n",
    "        \n",
    "        len_val = len(val_data)\n",
    "        model.eval()\n",
    "        \n",
    "        val_inputs=train_data[-train_window:,:]\n",
    "        val_list=[]\n",
    "        \n",
    "        model.eval()\n",
    "    #     fut_pred = 650\n",
    "        for n in range(len_val):\n",
    "            seq = val_inputs[-train_window:].to(device)\n",
    "            with torch.no_grad():\n",
    "                model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                              torch.zeros(1, 1, model.hidden_layer_size))\n",
    "                seq = seq.unsqueeze(0)\n",
    "#               ConvLSTM\n",
    "#                 seq = seq.unsqueeze(0)\n",
    "#                 seq = seq.unsqueeze(2)\n",
    "                y_pred=model(seq)\n",
    "                # Convlstm\n",
    "#                 y_pred = torch.tensor(y_pred).view(-1,1)\n",
    "#                 y_pred = y_pred.to(device)\n",
    "                # Convlstm\n",
    "#                 val_list.append(y_pred)\n",
    "            \n",
    "                loss = loss_function(y_pred,val_data[n,0].view(1,-1))\n",
    "                add1 += loss\n",
    "                temp=copy.deepcopy(val_data[n]).view(1,-1)\n",
    "\n",
    "                val_inputs=torch.cat((val_inputs,temp),0)\n",
    "                val_list.append(np.array(y_pred.cpu())[0])\n",
    "        \n",
    "#         plt.plot(np.array(val_data.cpu())[:,0],color=\"red\",label='real value')\n",
    "#         plt.plot(val_list,label='prediction')\n",
    "#         plt.show()\n",
    "        \n",
    "        loss_val=add1/len_val\n",
    "        loss_val_dic[attr].append(loss_val)\n",
    "        if loss_val<flag:\n",
    "            flag = loss_val\n",
    "            best_model = model\n",
    "            torch.save(best_model.state_dict(), path)\n",
    "        print(f'epoch: {i:3}  train_loss:{loss_train:10.8f} val_loss:{loss_val:10.8f}')\n",
    "    model = best_model\n",
    "    print('----------------------')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def test_model(attr,model):\n",
    "    temp=torch.cat((train_data,val_data))\n",
    "    print(temp.shape)\n",
    "    test_inputs=temp[-train_window:,:]\n",
    "    print(test_inputs.shape)\n",
    "\n",
    "    fut_pred = len(test_data)\n",
    "    test_list=[]\n",
    "    test_results=copy.deepcopy(test_data)\n",
    "\n",
    "    model.eval()\n",
    "    for i in range(fut_pred):\n",
    "        seq = test_inputs[-train_window:].to(device)\n",
    "        with torch.no_grad():\n",
    "#             model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "#                           torch.zeros(1, 1, model.hidden_layer_size))\n",
    "            seq = seq.unsqueeze(0)\n",
    "            y_pred=model(seq)\n",
    "            \n",
    "            temp=copy.deepcopy(test_data[i]).view(1,-1)\n",
    "#             temp[0]=y_pred\n",
    "            test_inputs=torch.cat((test_inputs,temp),0)\n",
    "            test_results[i,0]=y_pred\n",
    "\n",
    "    local_path = ''\n",
    "\n",
    "\n",
    "    actual_predictions = scaler.inverse_transform(np.array(test_results.cpu()))\n",
    "\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.rcParams['font.sans-serif'] = ['Times New Roman']\n",
    "    plt.rcParams['figure.figsize'] = (10,5)\n",
    "    x = np.arange(len(train_data)+len(val_data), len(dataset), 1)\n",
    "\n",
    "    data_target = origin[:,0]\n",
    "    plt.plot(data_target[len(dataset)-len(test_data):] ,'r',label='Test data')\n",
    "    plt.plot(actual_predictions[:,0],label='Prediction')\n",
    "\n",
    "#     plt.title('days vs '+attr)\n",
    "    plt.legend(loc='upper right',fontsize=12)\n",
    "    plt.xlabel('Time',fontsize=16)\n",
    "    plt.ylabel('AQI',fontsize=16)\n",
    "    plt.tick_params(labelsize=12)\n",
    "    path = local_path + attr + '_aqi.png'\n",
    "    plt.savefig(path,dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "    y_true=data_target[len(dataset)-len(test_data):]\n",
    "    y_pred=actual_predictions[:,0]\n",
    "\n",
    "    print(\"mae:\", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"mse:\", mean_squared_error(y_true, y_pred))\n",
    "    print(\"rmse:\", sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    print(\"r2 score:\", r2_score(y_true, y_pred))\n",
    "    print(\"mape:\", mean_absolute_percentage_error(y_true, y_pred))\n",
    "\n",
    "    y_pred=pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = 30, epoch=400\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "class BP(nn.Module):\n",
    "    def __init__(self,input_size=24,hidden_layer_size=100,output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1=nn.Linear(input_size*train_window,hidden_layer_size)\n",
    "        self.linear2=nn.Linear(hidden_layer_size,output_size)\n",
    "        \n",
    "    def forward(self,input_seq):\n",
    "        input_seq = input_seq.permute(0,2,1)\n",
    "        out = self.flatten(input_seq)\n",
    "        out=self.linear1(out)\n",
    "#         out=torch.tanh(out)\n",
    "        predictions = self.linear2(out)\n",
    "#         print(predictions)\n",
    "        return predictions\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "device=torch.device('cuda')\n",
    "model_aqi = BP().to(device)\n",
    "loss_function=nn.MSELoss()\n",
    "optimizer_aqi = torch.optim.Adam(model_aqi.parameters(), lr=0.0001)\n",
    "scheduler = StepLR(optimizer_aqi, step_size=5, gamma=0.95)\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BP\n",
    "best_model = train_model('BP',model_aqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BP\n",
    "test_model('BP',model_aqi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size=77,hidden_layer_size=256,output_size=1,num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.rnn=nn.RNN(input_size,hidden_layer_size,num_layers)\n",
    "        self.linear1=nn.Linear(hidden_layer_size,hidden_layer_size)\n",
    "        self.linear2=nn.Linear(hidden_layer_size,output_size)\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "#         self.hidden_cell=(torch.zeros(num_layers,1,self.hidden_layer_size),torch.zeros(num_layers,1,self.hidden_layer_size))\n",
    "        init_rnn(self.rnn,'xavier')\n",
    "        \n",
    "    def forward(self,input_seq):\n",
    "        input_seq = input_seq.permute(1,0,2)\n",
    "        rnn_out, self.hidden_cell = self.rnn(input_seq)\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        out=self.linear1(rnn_out)\n",
    "#         out=torch.tanh(out)\n",
    "        predictions = self.linear2(out)\n",
    "        return predictions[-1]\n",
    "\n",
    "\n",
    "def init_rnn(x, type='uniform'):\n",
    "    for layer in x._all_weights:\n",
    "        for w in layer:\n",
    "            if 'weight' in w:\n",
    "                if type == 'xavier':\n",
    "                    init.xavier_normal_(getattr(x, w))\n",
    "                elif type == 'uniform':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.uniform_(getattr(x, w), -stdv, stdv)\n",
    "                elif type == 'normal':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.normal_(getattr(x, w), .0, stdv)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "                    \n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "device=torch.device('cuda')\n",
    "model_aqi = RNN().to(device)\n",
    "loss_function=nn.MSELoss()\n",
    "optimizer_aqi = torch.optim.Adam(model_aqi.parameters(), lr=0.0001)\n",
    "scheduler = StepLR(optimizer_aqi, step_size=5, gamma=0.95)\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "best_model = train_model('RNN',model_aqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "test_model('RNN',best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,input_size=77,hidden_layer_size=256,output_size=1,num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.lstm=nn.LSTM(input_size,hidden_layer_size,num_layers)\n",
    "        self.linear1=nn.Linear(hidden_layer_size,hidden_layer_size)\n",
    "        self.linear2=nn.Linear(hidden_layer_size,output_size)\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "#         self.hidden_cell=(torch.zeros(num_layers,1,self.hidden_layer_size),torch.zeros(num_layers,1,self.hidden_layer_size))\n",
    "        init_rnn(self.lstm,'xavier')\n",
    "        \n",
    "    def forward(self,input_seq):\n",
    "        input_seq = input_seq.permute(1,0,2)\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out=self.linear1(lstm_out)\n",
    "#         out=torch.tanh(out)\n",
    "        predictions = self.linear2(out)\n",
    "        return predictions[-1]\n",
    "\n",
    "\n",
    "def init_rnn(x, type='uniform'):\n",
    "    for layer in x._all_weights:\n",
    "        for w in layer:\n",
    "            if 'weight' in w:\n",
    "                if type == 'xavier':\n",
    "                    init.xavier_normal_(getattr(x, w))\n",
    "                elif type == 'uniform':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.uniform_(getattr(x, w), -stdv, stdv)\n",
    "                elif type == 'normal':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.normal_(getattr(x, w), .0, stdv)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "                    \n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "device=torch.device('cuda')\n",
    "model_aqi = LSTM().to(device)\n",
    "loss_function=nn.MSELoss()\n",
    "optimizer_aqi = torch.optim.Adam(model_aqi.parameters(), lr=0.0001)\n",
    "scheduler = StepLR(optimizer_aqi, step_size=5, gamma=0.95)\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "best_model = train_model('LSTM',model_aqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "test_model('LSTM',best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = 30\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,input_size=77,hidden_layer_size=100,output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.conv1d1=nn.Conv1d(in_channels=input_size,out_channels =16, kernel_size = 1)\n",
    "        self.conv1d2=nn.Conv1d(in_channels=16,out_channels =32, kernel_size = 1)\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1=nn.Linear(16*train_window,hidden_layer_size)\n",
    "        self.linear2=nn.Linear(hidden_layer_size,output_size)\n",
    "        \n",
    "    def forward(self,input_seq):\n",
    "        input_seq = input_seq.permute(0,2,1)\n",
    "        input_seq = self.conv1d1(input_seq)\n",
    "        input_seq = self.conv1d2(input_seq)\n",
    "        input_seq = self.maxpool1d(input_seq)\n",
    "        out = self.flatten(input_seq)\n",
    "        out=self.linear1(out)\n",
    "#         out=torch.tanh(out)\n",
    "        predictions = self.linear2(out)\n",
    "#         print(predictions)\n",
    "        return predictions\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "device=torch.device('cuda')\n",
    "model_aqi = CNN().to(device)\n",
    "loss_function=nn.MSELoss()\n",
    "optimizer_aqi = torch.optim.Adam(model_aqi.parameters(), lr=0.0001)\n",
    "scheduler = StepLR(optimizer_aqi, step_size=5, gamma=0.95)\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "best_model = train_model('CNN',model_aqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "test_model('CNN',best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN-LSTM with attention\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self,input_size=20,hidden_layer_size=256,output_size=1,num_layers=2):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.output_size=output_size\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.num_layers=num_layers\n",
    "\n",
    "        self.conv1d1=nn.Conv1d(in_channels=input_size,out_channels =32, kernel_size = 1)\n",
    "        self.conv1d2=nn.Conv1d(in_channels=32,out_channels =64, kernel_size = 1)\n",
    "        self.activation=nn.ELU()\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size=2)\n",
    "#         self.lstm1=nn.LSTM(32,hidden_layer_size1,num_layers)\n",
    "        self.lstm=nn.LSTM(64,hidden_layer_size,num_layers,dropout=0.2)\n",
    "        \n",
    "        self.linear1=nn.Linear(self.hidden_layer_size,self.hidden_layer_size)\n",
    "        self.linear2=nn.Linear(self.hidden_layer_size,output_size)\n",
    "        init_rnn(self.lstm,'xavier')\n",
    "#         init_rnn(self.lstm2,'xavier')\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "        self.attn_weights=None\n",
    "\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "\n",
    "        lstm_output = lstm_output.permute(1, 0, 2)\n",
    "#         lstm_output [1, 59 ,50]\n",
    "#         final_state torch.Size([2, 1, 50])\n",
    "#         print(final_state.shape)\n",
    "        hidden = final_state.view(-1,self.hidden_layer_size, 1)   # hidden : [batch_size, n_hidden * num_directions, 1(=n_layer)]\n",
    "#         torch.Size([1, 50, 2])\n",
    "        self.attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, n_step]\n",
    "#         print(self.attn_weights.shape)\n",
    "        soft_attn_weights = torch.tanh(self.attn_weights)\n",
    "#         print(soft_attn_weights.shape)\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return context # context : [batch_size, n_hidden * num_directions]\n",
    "    \n",
    "    def forward(self,input_seq):\n",
    "        batch_size = input_seq.shape[0]\n",
    "        input_seq=input_seq.permute(0,2,1)\n",
    "        input_seq=self.conv1d1(input_seq)\n",
    "        input_seq=self.activation(input_seq)\n",
    "        input_seq=self.conv1d2(input_seq)\n",
    "        input_seq=self.activation(input_seq)\n",
    "        input_seq=input_seq.permute(2,0,1)\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # torch.Size([59, 1, 50])\n",
    "        # torch.Size([1, 50, 2])\n",
    "#         print(lstm_out.shape)\n",
    "        out=self.linear1(lstm_out)\n",
    "        # torch.Size([2, 50])\n",
    "#         print(out.shape)\n",
    "        predictions = self.linear2(out)\n",
    "#         print(predictions.shape)\n",
    "        # torch.Size([2, 1])\n",
    "#         predictions = predictions[:, -1, :]\n",
    "        return predictions[-1]\n",
    "\n",
    "def init_rnn(x, type='uniform'):\n",
    "    for layer in x._all_weights:\n",
    "        for w in layer:\n",
    "            if 'weight' in w:\n",
    "                if type == 'xavier':\n",
    "                    init.xavier_normal_(getattr(x, w))\n",
    "                elif type == 'uniform':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.uniform_(getattr(x, w), -stdv, stdv)\n",
    "                elif type == 'normal':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.normal_(getattr(x, w), .0, stdv)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "device=torch.device('cuda')\n",
    "model_aqi = CNNLSTM().to(device)\n",
    "loss_function=nn.MSELoss()\n",
    "optimizer_aqi = torch.optim.Adam(model_aqi.parameters(), lr=0.0001)\n",
    "scheduler = StepLR(optimizer_aqi, step_size=5, gamma=0.95)\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNLSTM\n",
    "best_model = train_model('CNNLSTM',model_aqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNLSTM\n",
    "test_model('CNNLSTM',best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTNet\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "class LSTNet(nn.Module):\n",
    "    def __init__(self,input_size=77,hidden_layer_size=256,output_size=1,num_layers=2,window=8):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.output_size=output_size\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.num_layers=num_layers\n",
    "        self.hidR = 32 \n",
    "        self.hidC = 32 \n",
    "        self.hidS = 3 \n",
    "        self.kernel_size = 1\n",
    "\n",
    "        self.conv1d1=nn.Conv1d(in_channels=input_size,out_channels =self.hidC, kernel_size = self.kernel_size)\n",
    "        self.conv1d2=nn.Conv1d(in_channels=self.hidC,out_channels =self.hidR, kernel_size = self.kernel_size)\n",
    "        self.activation=nn.ELU()\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size=2)\n",
    "#         self.lstm1=nn.LSTM(32,hidden_layer_size1,num_layers)\n",
    "        self.GRU1=nn.GRU(self.hidR,self.hidden_layer_size,num_layers,dropout=0.2)\n",
    "        init_rnn(self.GRU1,'xavier')\n",
    "#         init_rnn(self.lstm2,'xavier')\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "        self.attn_weights=None\n",
    "        self.skip = 4 \n",
    "        self.pt = int((window - self.kernel_size)/self.skip) \n",
    "        self.hw = 3 \n",
    "        if (self.skip > 0):\n",
    "            self.GRUskip = nn.GRU(self.hidC, self.hidS)\n",
    "            self.linear1 = nn.Linear(self.hidden_layer_size + self.skip * self.hidS, self.output_size)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(self.hidden_layer_size, self.output_size)\n",
    "        if (self.hw > 0):\n",
    "            self.highway = nn.Linear(self.hw, self.output_size)\n",
    "        self.linear2 = nn.Linear(input_size, self.output_size)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self,input_seq):\n",
    "        batch_size = input_seq.shape[0]\n",
    "        c=input_seq.permute(0,2,1)\n",
    "        c=self.conv1d1(c)\n",
    "        c=self.activation(c)\n",
    "        c=self.conv1d2(c)\n",
    "        c=self.activation(c)\n",
    "        r=c.permute(2,0,1)\n",
    "        GRU_out, self.hidden_cell = self.GRU1(r)\n",
    "        GRU_out = GRU_out[-1]\n",
    "        GRU_out = self.dropout(GRU_out)\n",
    "\n",
    "        #skip-rnn\n",
    "        if (self.skip > 0):\n",
    "            s = c[:,:, int(-self.pt * self.skip):].contiguous()\n",
    "            s = s.view(batch_size, self.hidC, self.pt, self.skip)\n",
    "            s = s.permute(2,0,3,1).contiguous()\n",
    "            s = s.view(self.pt, batch_size * self.skip, self.hidC)\n",
    "            _, s = self.GRUskip(s)\n",
    "            s = s.view(batch_size, self.skip * self.hidS)\n",
    "            s = self.dropout(s)\n",
    "            GRU_out = torch.cat((GRU_out,s),1)\n",
    "        \n",
    "        res = self.linear1(GRU_out)\n",
    "        \n",
    "        #highway\n",
    "        if (self.hw > 0):\n",
    "            z = input_seq[:, -self.hw:, :]\n",
    "            z = z.permute(0,2,1)\n",
    "            z = self.highway(z)\n",
    "            z = z.permute(1,0,2)\n",
    "            z = z[-1]\n",
    "            predictions = res + z\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "def init_rnn(x, type='uniform'):\n",
    "    for layer in x._all_weights:\n",
    "        for w in layer:\n",
    "            if 'weight' in w:\n",
    "                if type == 'xavier':\n",
    "                    init.xavier_normal_(getattr(x, w))\n",
    "                elif type == 'uniform':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.uniform_(getattr(x, w), -stdv, stdv)\n",
    "                elif type == 'normal':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.normal_(getattr(x, w), .0, stdv)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "device=torch.device('cuda')\n",
    "model_aqi = LSTNet().to(device)\n",
    "loss_function=nn.MSELoss()\n",
    "optimizer_aqi = torch.optim.Adam(model_aqi.parameters(), lr=0.0001)\n",
    "scheduler = StepLR(optimizer_aqi, step_size=5, gamma=0.95)\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTNet\n",
    "best_model = train_model('LSTNet', model_aqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTNet\n",
    "test_model('LSTNet',best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-1_1zW9wdih"
   },
   "source": [
    "# CNN-LSTM with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4151,
     "status": "ok",
     "timestamp": 1600295112216,
     "user": {
      "displayName": "Mingming Luo",
      "photoUrl": "",
      "userId": "03419135428294618807"
     },
     "user_tz": -60
    },
    "id": "HhfChsGuobK1"
   },
   "outputs": [],
   "source": [
    "#CNN-LSTM with attention\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "class CNNLSTMwithAttention(nn.Module):\n",
    "    def __init__(self,input_size=24,hidden_layer_size=256,output_size=1,num_layers=2):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.output_size=output_size\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.num_layers=num_layers\n",
    "\n",
    "        self.conv1d1=nn.Conv1d(in_channels=input_size,out_channels =32, kernel_size = 1)\n",
    "        self.conv1d2=nn.Conv1d(in_channels=32,out_channels =64, kernel_size = 1)\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size=2)\n",
    "#         self.lstm1=nn.LSTM(32,hidden_layer_size1,num_layers)\n",
    "        self.lstm=nn.LSTM(64,hidden_layer_size,num_layers,dropout=0.2)\n",
    "        \n",
    "        self.linear1=nn.Linear(self.hidden_layer_size,self.hidden_layer_size)\n",
    "        self.linear2=nn.Linear(self.hidden_layer_size,output_size)\n",
    "        init_rnn(self.lstm,'xavier')\n",
    "#         init_rnn(self.lstm2,'xavier')\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "        self.attn_weights=None\n",
    "\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "\n",
    "        lstm_output = lstm_output.permute(1, 0, 2)\n",
    "#         lstm_output [1, 59 ,50]\n",
    "#         final_state torch.Size([2, 1, 50])\n",
    "#         print(final_state.shape)\n",
    "        hidden = final_state.view(-1,self.hidden_layer_size, 1)   # hidden : [batch_size, n_hidden * num_directions, 1(=n_layer)]\n",
    "#         torch.Size([1, 50, 2])\n",
    "        self.attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, n_step]\n",
    "#         print(self.attn_weights.shape)\n",
    "        soft_attn_weights = torch.tanh(self.attn_weights)\n",
    "#         print(soft_attn_weights.shape)\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return context # context : [batch_size, n_hidden * num_directions]\n",
    "    \n",
    "    def forward(self,input_seq):\n",
    "#         print(input_seq.shape)\n",
    "        batch_size = input_seq.shape[0]\n",
    "        input_seq=input_seq.permute(0,2,1)\n",
    "        # torch.Size([1, 6, 60])\n",
    "        input_seq=self.conv1d1(input_seq)\n",
    "#         torch.Size([1, 100, 29])\n",
    "        input_seq=self.conv1d2(input_seq)\n",
    "#         torch.Size([1, 50, 28])\n",
    "#         input_seq=self.maxpool1d(input_seq)\n",
    "#         torch.Size([1, 50, 14])\n",
    "        input_seq=input_seq.permute(2,0,1)\n",
    "        # torch.Size([1, 59, 20])\n",
    "#         input_seq=input_seq.reshape(len(input_seq[0]),batch_size,input_seq.shape[2])\n",
    "        # torch.Size([59, 1, 20])\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # torch.Size([59, 1, 50])\n",
    "        attn_output = self.attention_net(lstm_out,self.hidden_cell[0][1].unsqueeze(0))\n",
    "        # torch.Size([1, 50, 2])\n",
    "        out=self.linear1(attn_output.view(-1,self.hidden_layer_size))\n",
    "        # torch.Size([2, 50])\n",
    "        predictions = self.linear2(out)\n",
    "        # torch.Size([2, 1])\n",
    "#         predictions = predictions[:, -1, :]\n",
    "        return predictions\n",
    "\n",
    "def init_rnn(x, type='uniform'):\n",
    "    for layer in x._all_weights:\n",
    "        for w in layer:\n",
    "            if 'weight' in w:\n",
    "                if type == 'xavier':\n",
    "                    init.xavier_normal_(getattr(x, w))\n",
    "                elif type == 'uniform':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.uniform_(getattr(x, w), -stdv, stdv)\n",
    "                elif type == 'normal':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.normal_(getattr(x, w), .0, stdv)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "device=torch.device('cuda')\n",
    "model_aqi = CNNLSTMwithAttention().to(device)\n",
    "loss_function=nn.MSELoss()\n",
    "optimizer_aqi = torch.optim.Adam(model_aqi.parameters(), lr=0.0001)\n",
    "scheduler = StepLR(optimizer_aqi, step_size=5, gamma=0.95)\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNLSTMwithAttention\n",
    "best_model = train_model('CNNLSTMwithAttention',model_aqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNLSTMwithAttention\n",
    "test_model('CNNLSTMwithAttention',best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bilstm with attention\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self,input_size=77,hidden_layer_size=256,output_size=1,num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.bilstm=nn.LSTM(input_size,hidden_layer_size,num_layers,bidirectional=True)\n",
    "        self.linear1=nn.Linear(hidden_layer_size*2,hidden_layer_size)\n",
    "        self.linear2=nn.Linear(hidden_layer_size,output_size)\n",
    "#         self.hidden_cell=(torch.zeros(num_layers*2,8,self.hidden_layer_size).to(device),torch.zeros(num_layers*2,8,self.hidden_layer_size).to(device))\n",
    "        self.num_layers=num_layers\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "        init_rnn(self.bilstm,'xavier')\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        lstm_output = lstm_output.permute(1, 0, 2)\n",
    "        batch_size = len(lstm_output)\n",
    "        hidden = final_state.view(batch_size, -1, 1)  \n",
    "        # hidden : [batch_size, n_hidden * num_directions(=2), n_layer(=1)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) \n",
    "        # attn_weights : [batch_size, n_step]\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        context = self.dropout(context)\n",
    "        return context\n",
    "\n",
    "    def forward(self,input_seq):\n",
    "        input_seq = input_seq.permute(1,0,2)\n",
    "        bilstm_out, self.hidden_cell = self.bilstm(input_seq)\n",
    "#         attn_output = self.attention_net(bilstm_out,self.hidden_cell[0])\n",
    "        # torch.Size([1, 50, 2])\n",
    "        bilstml_out=self.dropout(bilstm_out)\n",
    "        out=self.linear1(bilstm_out)\n",
    "        # torch.Size([2, 50])\n",
    "#         out=self.linear1(out.view(len(input_seq)*2, -1))\n",
    "#         out=torch.tanh(out)\n",
    "        predictions = self.linear2(out)\n",
    "        return predictions[-1]\n",
    "\n",
    "\n",
    "def init_rnn(x, type='uniform'):\n",
    "    for layer in x._all_weights:\n",
    "        for w in layer:\n",
    "            if 'weight' in w:\n",
    "                if type == 'xavier':\n",
    "                    init.xavier_normal_(getattr(x, w))\n",
    "                elif type == 'uniform':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.uniform_(getattr(x, w), -stdv, stdv)\n",
    "                elif type == 'normal':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.normal_(getattr(x, w), .0, stdv)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "device=torch.device('cuda')\n",
    "model_aqi = BiLSTM().to(device)\n",
    "loss_function=nn.MSELoss()\n",
    "optimizer_aqi = torch.optim.Adam(model_aqi.parameters(), lr=0.0001)\n",
    "scheduler = StepLR(optimizer_aqi, step_size=5, gamma=0.95)\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM\n",
    "best_model = train_model('BiLSTM',model_aqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM\n",
    "test_model('BiLSTM',best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bilstm with attention\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "class BiLSTMwithAttention(nn.Module):\n",
    "    def __init__(self,input_size=24,hidden_layer_size=256,output_size=1,num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.bilstm=nn.LSTM(input_size,hidden_layer_size,num_layers,bidirectional=True)\n",
    "        self.linear1=nn.Linear(hidden_layer_size*2,hidden_layer_size)\n",
    "        self.linear2=nn.Linear(hidden_layer_size,output_size)\n",
    "#         self.hidden_cell=(torch.zeros(num_layers*2,8,self.hidden_layer_size).to(device),torch.zeros(num_layers*2,8,self.hidden_layer_size).to(device))\n",
    "        self.num_layers=num_layers\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "        init_rnn(self.bilstm,'xavier')\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        lstm_output = lstm_output.permute(1, 0, 2)\n",
    "        batch_size = len(lstm_output)\n",
    "        hidden = final_state.view(batch_size, -1, 1)  \n",
    "        # hidden : [batch_size, n_hidden * num_directions(=2), n_layer(=1)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) \n",
    "        # attn_weights : [batch_size, n_step]\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        context = self.dropout(context)\n",
    "        return context\n",
    "    \n",
    "    def forward(self,input_seq):\n",
    "        input_seq = input_seq.permute(1,0,2)\n",
    "        bilstm_out, self.hidden_cell = self.bilstm(input_seq)\n",
    "        attn_output = self.attention_net(bilstm_out,self.hidden_cell[0])\n",
    "        # torch.Size([1, 50, 2])\n",
    "        out=self.linear1(attn_output.view(-1,self.hidden_layer_size*2))\n",
    "#         out=self.dropout(out)\n",
    "        # torch.Size([2, 50])\n",
    "#         out=self.linear1(out.view(len(input_seq)*2, -1))\n",
    "#         out=torch.tanh(out)\n",
    "        predictions = self.linear2(out)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def init_rnn(x, type='uniform'):\n",
    "    for layer in x._all_weights:\n",
    "        for w in layer:\n",
    "            if 'weight' in w:\n",
    "                if type == 'xavier':\n",
    "                    init.xavier_normal_(getattr(x, w))\n",
    "                elif type == 'uniform':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.uniform_(getattr(x, w), -stdv, stdv)\n",
    "                elif type == 'normal':\n",
    "                    stdv = 1.0 / math.sqrt(x.hidden_size)\n",
    "                    init.normal_(getattr(x, w), .0, stdv)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "device=torch.device('cuda')\n",
    "model_aqi = BiLSTMwithAttention().to(device)\n",
    "loss_function=nn.MSELoss()\n",
    "optimizer_aqi = torch.optim.Adam(model_aqi.parameters(), lr=0.0001)\n",
    "scheduler = StepLR(optimizer_aqi, step_size=5, gamma=0.95)\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM\n",
    "best_model = train_model('BiLSTMwithAttention',model_aqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM\n",
    "test_model('BiLSTMwithAttention',best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.get_cmap('Set3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_path = ''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.rcParams[\"font.sans-serif\"]=['Times New Roman']\n",
    "plt.rcParams[\"axes.unicode_minus\"]=False\n",
    "column = ['pm2.5','pm10','SO2','NO2','CO','O3']\n",
    "d_weight = [0.8702,0.6893,0.7150,0.7416,0.6655,0.7929]\n",
    "select = [0,2,3,4,5,6]\n",
    "colormap = plt.get_cmap('Set3')(select)\n",
    "\n",
    "\n",
    "for i in range(len(column)):\n",
    "    plt.bar(column[i],d_weight[i],width=0.5, color=colormap[i])\n",
    "for a,b in zip(column,d_weight): \n",
    "    plt.text(a,b,'%.2f'%b,ha='center',va='bottom',fontsize=16);\n",
    "\n",
    "plt.tick_params(labelsize=16)\n",
    "    \n",
    "plt.title(\"domain weight\", fontsize=16)\n",
    "plt.savefig(your_path, dpi=600)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstmWithAttention_multivar_sites.ipynb",
   "provenance": [
    {
     "file_id": "1bE9BR1VLYohcN5Ko1Xfqnulh5XF0Unyu",
     "timestamp": 1595599435653
    },
    {
     "file_id": "1IxQcZpOXeCS9CtXRmPjnFz7k5vwDTcqP",
     "timestamp": 1595516852382
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
